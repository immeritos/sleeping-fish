---
title: 'Interview Twin: Dveloping a Production-Ready LLM Application'
description: 'Interview Twin is an ongoing project aimed at creating a production-ready LLM-powered interview assistant. It serves both as a personal tool and a practical case study for building end-to-end AI applications using modern web and cloud technologies.'
date: '2025-11-01'
coverImage: '/projects/interview_twin_cover.jpg'
href: 'https://github.com/fanjingwenvi/interview-twin'
tags: ['LLM', 'RAG', 'Next.js', 'FastAPI','PostgreSQL', 'Qdrant', 'Docker', 'AWS']
---

## Architecture Diagram
*(to be added as the project evolves)*

## Why

1. Personal helper for interview preparation  
2. To demonstrate the process of developing a full end-to-end product
3. Networking, self-advertising, and learning in public

## What

System scope:
- The goal is to Production-ready and deployable on the cloud  — not just a demo.

References
- [LLM Engineer’s Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook)  
- [LLM Zoomcamp](https://github.com/DataTalksClub/llm-zoomcamp)  
- [How to Build Production-Ready RAG Systems](https://github.com/benitomartin/substack-newsletters-search-course/tree/main)

## How

### Tech Stack
- **Frontend:** Next.js.
- **Backend:**  A REST API backend containerized with Docker.  
- **Database:** PostgreSQL for structured data and Qdrant (or pgvector) for vector embeddings, supporting semantic search and retrieval.
- **Cloud Infrastructure:** Deployed on AWS using ECS Fargate, with potential usage of Lambda Docker or App Runner.  

### Dataset
The system integrates personal notes, interview reflections, and structured knowledge as a private dataset.

### Pipelines
1. **Feature Pipeline** – collect and structure data  
2. **Training Pipeline** – fine-tune and build embeddings  
3. **Inference Pipeline** – serve and query the model  
+ **LLM Ops** for monitoring and iteration.


